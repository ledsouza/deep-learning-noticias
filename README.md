## Classifica√ß√£o de Not√≠cias e Autocomplete de Texto üì∞

![Static Badge](https://img.shields.io/badge/Status-Finalizado-green)

## Descri√ß√£o

Este projeto visa construir dois modelos de Machine Learning: um para classificar not√≠cias em diferentes categorias e outro para realizar o autocomplete de texto, prevendo a pr√≥xima palavra em uma frase. O conjunto de dados fornecido consiste em not√≠cias de um site de not√≠cias, j√° pr√©-processadas e armazenadas em um arquivo CSV.

## Tecnologias Utilizadas

- Python
- Pandas
- Scikit-learn
- TensorFlow
- Keras

## Dicion√°rio de Dados

O arquivo CSV de dados cont√©m as seguintes colunas:

| Coluna        | Descri√ß√£o                                      | Tipo de Dado |
|---------------|-------------------------------------------------|--------------|
| ClassIndex    | √çndice da categoria da not√≠cia (0-3)            | Num√©rico     |
| T√≠tulo        | T√≠tulo da not√≠cia                               | Texto        |
| Descri√ß√£o     | Descri√ß√£o da not√≠cia                            | Texto        |
| Texto         | T√≠tulo e descri√ß√£o concatenados, separados por espa√ßo | Texto        |

## Descri√ß√£o Detalhada do Projeto

### Classifica√ß√£o de Not√≠cias

- **Pr√©-processamento:**
    - O campo `ClassIndex` foi ajustado para que os valores variem de 0 a 3, em vez de 1 a 4, para compatibilidade com o TensorFlow.
    - Uma nova coluna `Texto` foi criada, combinando o t√≠tulo e a descri√ß√£o da not√≠cia.
- **Separa√ß√£o de Dados:**
    - Os dados foram divididos em conjuntos de treino e teste usando `train_test_split` da biblioteca Scikit-learn, com uma propor√ß√£o de 80% para treino e 20% para teste.
- **Tokeniza√ß√£o:**
    - A coluna `Texto` foi tokenizada usando `TextVectorization` do Keras, com um vocabul√°rio de tamanho `VOCAB_SIZE`.
- **Modelagem:**
    - Diversas arquiteturas de modelos de classifica√ß√£o foram experimentadas, incluindo:
        - **Modelo 1:** Uma rede neural simples com camadas `Embedding`, `GlobalAveragePooling1D` e `Dense`.
        - **Modelo 2:** Uma rede neural convolucional (CNN) com camadas `Embedding`, `Conv1D`, `MaxPooling1D`, `GlobalAveragePooling1D`, `Dropout` e `Dense`.
        - **Modelo com camadas LSTM:** Uma rede neural recorrente (RNN) com camadas `Embedding`, `Bidirectional(LSTM)`, `Dense` e `Dropout`.
    - Os modelos foram compilados com o otimizador `Adam`, a fun√ß√£o de perda `sparse_categorical_crossentropy` e a m√©trica `accuracy`.
- **Treinamento:**
    - Os modelos foram treinados com os dados de treino, utilizando o conjunto de teste para valida√ß√£o.
    - O desempenho do modelo foi avaliado usando a acur√°cia e a perda nos conjuntos de treino e valida√ß√£o.
- **Avalia√ß√£o:**
    - Uma matriz de confus√£o foi gerada para visualizar o desempenho do modelo em cada categoria.

### Autocomplete de Texto

- **Amostragem:**
    - Uma amostra menor dos dados foi utilizada para acelerar o treinamento do modelo de autocomplete.
- **Tokeniza√ß√£o:**
    - O texto foi tokenizado usando `TextVectorization` do Keras.
- **Pr√©-processamento:**
    - As sequ√™ncias de entrada foram preparadas da seguinte forma:
        - Remo√ß√£o de zeros √† direita de cada sequ√™ncia.
        - Adi√ß√£o de padding √† esquerda para garantir o mesmo comprimento.
        - Truncamento de sequ√™ncias longas.
        - Remo√ß√£o de sequ√™ncias repetidas.
- **Modelagem:**
    - Uma rede neural recorrente (RNN) com camadas `Embedding`, `Bidirectional(LSTM)`, `Dense`, `BatchNormalization` e `Dropout` foi utilizada.
    - O modelo foi compilado com a fun√ß√£o de perda `categorical_crossentropy`, o otimizador `Adam` e a m√©trica `accuracy`.
- **Treinamento:**
    - O modelo foi treinado com as sequ√™ncias de entrada preparadas.
- **Previs√£o:**
    - Uma fun√ß√£o `predict_next_words` foi criada para prever as pr√≥ximas palavras mais prov√°veis, dado um texto de entrada.
